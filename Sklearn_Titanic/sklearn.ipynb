{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Titanic with Scikit-Learn</h1>\n",
    "<h1 align=\"center\"><font size = 5>Zach Chase</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin, TransformerMixin\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custon Transformer Class for Titanic\n",
    "Writing custom scikit-learn transformers is a convenient way to organize the data\n",
    "cleaning process. Consider the data in titanic.csv, which contains information about passengers on the maiden voyage of the RMS Titanic in 1912. Write a custom transformer class to\n",
    "clean this data, implementing the transform() method as follows:\n",
    "1. Extract a copy of data frame with just the \"Pclass\", \"Sex\", and \"Age\" columns.\n",
    "2. Replace NaN values in the \"Age\" column (of the copied data frame) with the mean age.\n",
    "The mean age of the training data should be calculated in fit() and used in transform()\n",
    "(compare this step to using sklearn.preprocessing.Imputer).\n",
    "3. Convert the \"Pclass\" column datatype to pandas categoricals (pd.CategoricalIndex).\n",
    "4. Use pd.get_dummies() to convert the categorical columns to multiple binary columns\n",
    "(compare this step to using sklearn.preprocessing.OneHotEncoder).\n",
    "5. Cast the result as a NumPy array and return it.\n",
    "Ensure that your transformer matches scikit-learn conventions (it inherits from the correct base\n",
    "classes, fit() returns self, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y =None):\n",
    "        \n",
    "        #Calculate mean age\n",
    "        self.mean_age = X.Age.mean()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        #Fill Na for Age\n",
    "        X.Age = X.Age.fillna(self.mean_age)\n",
    "        \n",
    "        #Get dummies for sex\n",
    "        X.Sex = pd.get_dummies(X.Sex)\n",
    "        #Pclass_types = ['1','2','3']\n",
    "        #X.Pclass.astype(\"category\", categories = Pclass_types).cat.codes\n",
    "        \n",
    "        #Get dummies for Pclass\n",
    "        X.Pclass = pd.CategoricalIndex(X.Pclass)\n",
    "        X.Pclass = pd.get_dummies(df.Pclass, drop_first=True)\n",
    "        #X.Pclass = pd.Categorical(X.Pclass)\n",
    "        #X.Pclass = X.Pclass.cat.codes\n",
    "        \n",
    "        X = X[[\"Age\", \"Sex\", \"Pclass\"]]\n",
    "        return X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Read the data from titanic.csv with pd.read_csv(). The \"Survived\" column\n",
    "indicates which passengers survived, so the entries of the column are the labels that we would\n",
    "like to predict. Drop any rows in the raw data that have NaN values in the \"Survived\" column,\n",
    "then separate the column from the rest of the data. Split the data and labels into training and\n",
    "testing sets. Use the training data to fit a transformer from Problem 1, then use that transformer\n",
    "to clean the training set, then the testing set. Finally, train a LogisticRegressionClassifier\n",
    "and a RandomForestClassifier on the cleaned training data, and score them using the cleaned\n",
    "test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Score \t:0.7591463414634146\n",
      "Random Forest Score \t\t:0.75\n"
     ]
    }
   ],
   "source": [
    "# read the data from filename\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "# drop rows that have NaN values in the survived column\n",
    "df = df.dropna(subset = [\"Survived\"])\n",
    "\n",
    "# separate survived column from rest of data\n",
    "y = df[\"Survived\"]\n",
    "X = df.drop([\"Survived\"], axis = 1)\n",
    "\n",
    "# split data and labels into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# use the train data to fit a transformer\n",
    "titanic = TitanicTransformer()\n",
    "\n",
    "# use the transformer to clean the training and test set\n",
    "titanicX_train = titanic.fit(X_train)\n",
    "cleanTitanicX_train = titanicX_train.transform(X_train)\n",
    "\n",
    "titanicX_test = titanic.fit(X_test)\n",
    "cleanTitanicX_test = titanicX_test.transform(X_test)\n",
    "\n",
    "# train a log reg classifier\n",
    "mylogreg = LogisticRegression().fit(cleanTitanicX_train, y_train)\n",
    "\n",
    "# train a random for classifier\n",
    "myRF = RandomForestClassifier().fit(cleanTitanicX_train, y_train)\n",
    "\n",
    "# score both classifiers using the cleaned test set\n",
    "logPredictions = mylogreg.predict(cleanTitanicX_test)\n",
    "logScore = mylogreg.score(cleanTitanicX_test, y_test)\n",
    "print('Logistic Regression Score \\t:{}'.format(logScore))\n",
    "\n",
    "rfPredictions = myRF.predict(cleanTitanicX_test)\n",
    "rfScore = myRF.score(cleanTitanicX_test, y_test)\n",
    "print('Random Forest Score \\t\\t:{}'.format(rfScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Use classification_report() to score your classifiers from Problem 2. Next,\n",
    "do a grid search for each classifier (using only the cleaned training data), varying at least two\n",
    "hyperparameters for each kind of model. Use classification_report() to score the resulting\n",
    "best estimators with the cleaned test data. Try changing the hyperparameter spaces or scoring\n",
    "metrics so that each grid search yields a better estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 3 Logistic Regression Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.84      0.81       204\n",
      "         1.0       0.70      0.63      0.66       124\n",
      "\n",
      "    accuracy                           0.76       328\n",
      "   macro avg       0.75      0.73      0.74       328\n",
      "weighted avg       0.76      0.76      0.76       328\n",
      "\n",
      "Problem 3 Random Forest Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.82      0.80       204\n",
      "         1.0       0.68      0.64      0.66       124\n",
      "\n",
      "    accuracy                           0.75       328\n",
      "   macro avg       0.73      0.73      0.73       328\n",
      "weighted avg       0.75      0.75      0.75       328\n",
      "\n",
      "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression Classification Report\n",
      "{'penalty': 'l1', 'solver': 'liblinear'} 0.7869462419113986\n",
      "Fitting 4 folds for each of 16 candidates, totalling 64 fits\n",
      "Best Random Forest Classification Report\n",
      "{'min_impurity_split': 1e-07, 'n_estimators': 150} 0.7818773851003816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  64 out of  64 | elapsed:    1.7s finished\n"
     ]
    }
   ],
   "source": [
    "# print classification report for log reg from previous problem\n",
    "print('Logistic Regression Classification Report')\n",
    "print(classification_report(y_test, logPredictions))\n",
    "\n",
    "# print classification report for random forest from previous problem\n",
    "print('Random Forest Classification Report')\n",
    "print(classification_report(y_test, rfPredictions))\n",
    "\n",
    "# grid search over 2+ hyperparameters for log reg\n",
    "gsLogReg = LogisticRegression()\n",
    "log_grid = {\"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "           \"penalty\": ['l1', 'l2', 'elasticnet', None]}\n",
    "log_gs = GridSearchCV(gsLogReg, log_grid, cv=4, n_jobs=-1, verbose=1)\n",
    "log_gs.fit(cleanTitanicX_train, y_train)\n",
    "\n",
    "# print classification report for best estimation\n",
    "print('Best Logistic Regression Classification Report')\n",
    "print(log_gs.best_params_, log_gs.best_score_)\n",
    "\n",
    "# grid search over 2+ hyperparameters for random forests\n",
    "gsRF = RandomForestClassifier()\n",
    "rf_grid = {\"n_estimators\" : [10, 50, 100, 150],\n",
    "           \"min_impurity_split\": [1e-5, 1e-6, 1e-7, 1e-8]}\n",
    "rf_gs = GridSearchCV(gsRF, rf_grid, cv=4, n_jobs=-1, verbose=1)\n",
    "rf_gs.fit(cleanTitanicX_train, y_train)\n",
    "\n",
    "\n",
    "# print classification report for best estimation\n",
    "print('Best Random Forest Classification Report')\n",
    "print(rf_gs.best_params_, rf_gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "Make a pipeline with at least two transformers to further process the Titanic\n",
    "dataset. Do a gridsearch on the pipeline and report the hyperparameters of the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logReg__solver': 'newton-cg', 'logReg__tol': 0.001, 'robust__with_centering': True, 'robust__with_scaling': True, 'scaler__with_mean': True, 'scaler__with_std': True}\n",
      "0.786931523878587\n"
     ]
    }
   ],
   "source": [
    "# make a pipeline with 2+ tranformers\n",
    "pipe = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                 (\"robust\", RobustScaler()),\n",
    "                 (\"logReg\", LogisticRegression())])\n",
    "\n",
    "# grid search on the pipeline\n",
    "pipe_param_grid = {\"scaler__with_mean\":[True, False],\n",
    "                  \"scaler__with_std\": [True, False],\n",
    "                   \"logReg__solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                   \"logReg__tol\": [1e-3, 1e-4, 1e-5],\n",
    "                  \"robust__with_centering\": [True, False],\n",
    "                  \"robust__with_scaling\": [True, False]}\n",
    "\n",
    "pipe_gs = GridSearchCV(pipe,pipe_param_grid).fit(cleanTitanicX_train, y_train)\n",
    "\n",
    "# report hyperparameters of the best estimator\n",
    "print(pipe_gs.best_params_, pipe_gs.best_score_, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Thanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very special thanks to the Brigham Young University ACME program for their guidance on this project. For further details, please visit https://acme.byu.edu/00000179-afb2-d74f-a3ff-bfbb157c0000/scikit19-pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
